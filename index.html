<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Abinit-medusa by roguephysicist</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Abinit-medusa</h1>
        <p>A guide for building ABINIT on Medusa</p>

        <p class="view"><a href="https://github.com/roguephysicist/abinit-medusa">View the Project on GitHub <small>roguephysicist/abinit-medusa</small></a></p>


        <ul>
          <li><a href="https://github.com/roguephysicist/abinit-medusa/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/roguephysicist/abinit-medusa/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/roguephysicist/abinit-medusa">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="instructions-for-building-abinit-on-medusa" class="anchor" href="#instructions-for-building-abinit-on-medusa" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Instructions for building ABINIT on Medusa</h1>

<p>This is a detailed guide for building highly optimized binaries for ABINIT on
Medusa. We will make extensive use of the Intel MPI and MKL libraries. This
streamlines the process considerably; combined with processor-specific
optimization, the resulting binaries are are extremely fast without sacrificing
accuracy.</p>

<p>As the Intel MPI and MKL libraries are already installed and optimized for the
system, we do not need to build any intermediate software. If you choose not to
use these libraries, then you will have to build your own external libraries for
FFTW, LINALG, and MPI, or default to the internal connectors that come with
ABINIT. However, this is outside the scope of this guide.</p>

<h2>
<a id="procedure" class="anchor" href="#procedure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Procedure</h2>

<p>We will use the adjoining <code>abinit-x.x.x.ac</code> configure files. Choose the
appropriate one for your version of ABINIT. This file will greatly facilitate
the build process as it contains all the correct build options and paths. Review
this file carefully, as it contains a lot of useful information. If you wish to
add more connectors or experiment with other options, I recommend doing so using
this file as most of the variables are extensively documented.</p>

<p><strong>Typically, the only thing you will need to modify is the <code>prefix</code>. Set this to
wherever you want to install ABINIT.</strong></p>

<h3>
<a id="step-0---prepare-the-build-environment" class="anchor" href="#step-0---prepare-the-build-environment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 0 - Prepare the build environment</h3>

<p>You can compile from any node in Medusa. The appropriate environment should be
loaded by default for common users in the <code>medusa</code>, <code>fat</code>, and <code>hexa</code> nodes.
Verify this by checking your <code>$PATH</code>. You should see the paths for the compiler
binaries (<code>.../linux/bin/intel64</code>) and the MPI binaries
(<code>.../linux/mpi/intel64/bin)</code>. Verify that you are able to execute <code>mpiifort</code>
directly. Lastly, verify that the <code>${MKLROOT}</code> and <code>${I_MPI_ROOT}</code> paths are
correct, and match those in your <code>$PATH</code>.</p>

<p><em>If you want to build ABINIT system-wide, you will need to build as the
super-user. By default, entering <code>root</code> does not have the proper environment
loaded. Load it with</em></p>

<div class="highlight highlight-source-shell"><pre>module load intel-compilers/16.2.181 intel-mpi/5.1.3.181 intel-mkl/16.2.181</pre></div>

<p><em>When compiling as root, it is best to only compile from the master node.</em></p>

<p>It is convenient to have the following variables:</p>

<div class="highlight highlight-source-shell"><pre>CC=<span class="pl-s"><span class="pl-pds">"</span>icc<span class="pl-pds">"</span></span>
CXX=<span class="pl-s"><span class="pl-pds">"</span>icpc<span class="pl-pds">"</span></span>
FC=<span class="pl-s"><span class="pl-pds">"</span>ifort<span class="pl-pds">"</span></span>
F77=<span class="pl-s"><span class="pl-pds">"</span>ifort<span class="pl-pds">"</span></span></pre></div>

<p>Check these and set them if not.</p>

<h3>
<a id="step-1---prepare-the-source-code" class="anchor" href="#step-1---prepare-the-source-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 1 - Prepare the source code</h3>

<p>You can find the tarballs for ABINIT and many other programs in
<code>/opt/science/tarballs/</code>.</p>

<p>Extract the ABINIT source code.</p>

<div class="highlight highlight-source-shell"><pre>tar -xvf /opt/science/tarballs/abinit-7.10.5.tar.gz
<span class="pl-c1">cd</span> abinit-7.10.5</pre></div>

<p>We will modify the configure script to match our new compilers. This will
suppress many useless notifications and allow the build system to correctly set
the MPI compilers.</p>

<div class="highlight highlight-source-shell"><pre>sed -i -e <span class="pl-s"><span class="pl-pds">'</span>s/vec-report0/qopt-report=0/g<span class="pl-pds">'</span></span> \
       -e <span class="pl-s"><span class="pl-pds">'</span>s/mpicc/mpiicc/g<span class="pl-pds">'</span></span> \
       -e <span class="pl-s"><span class="pl-pds">'</span>s/mpicxx/mpiicpc/g<span class="pl-pds">'</span></span> \
       -e <span class="pl-s"><span class="pl-pds">'</span>s/mpif90/mpiifort/g<span class="pl-pds">'</span></span> configure</pre></div>

<p>I recommend creating a new directory for building. This makes for easier clean
up and for testing different compile options.</p>

<pre><code>mkdir -p build &amp;&amp; cd build
</code></pre>

<p>Copy the appropriate <code>abinit-x.x.x.ac</code> file to this directory, and rename it to
<code>medusa.ac</code> or whatever the hostname is of the node your are in.</p>

<div class="highlight highlight-source-shell"><pre>cp abinit-x.x.x.ac <span class="pl-smi">${HOSTNAME}</span>.ac</pre></div>

<h3>
<a id="step-2---configure-and-build" class="anchor" href="#step-2---configure-and-build" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 2 - Configure and build</h3>

<p>Initiate the configuration process by running</p>

<div class="highlight highlight-source-shell"><pre>../configure</pre></div>

<p>The script will read from the <code>.ac</code> file and set up the build system
accordingly. Review the process carefully and check for any mistakes or
problems. If everything goes as planned, it should produce this summary:</p>

<pre><code>Summary of important options:

  * C compiler      : intel version 16.0
  * Fortran compiler: intel version 16.0
  * architecture    : intel xeon (64 bits)

  * debugging       : basic
  * optimizations   : yes

  * OpenMP enabled  : no (collapse: ignored)
  * MPI    enabled  : yes
  * MPI-IO enabled  : yes
  * GPU    enabled  : no (flavor: none)

  * TRIO   flavor = none
  * TIMER  flavor = abinit (libs: ignored)
  * LINALG flavor = mkl (libs: auto-detected)
  * ALGO   flavor = none (libs: ignored)
  * FFT    flavor = fftw3-mkl (libs: auto-detected)
  * MATH   flavor = none (libs: ignored)
  * DFT    flavor = none
</code></pre>

<p>You are now ready to build the software. You should parallelize this process to
significantly reduce compile time. For instance,</p>

<div class="highlight highlight-source-shell"><pre>make multi multi_nprocs=8</pre></div>

<p>will use 8 processor cores. <strong>For this level of optimization, compiling will
take roughly 35 minutes on 8 cores.</strong></p>

<h3>
<a id="step-3---testing" class="anchor" href="#step-3---testing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 3 - Testing</h3>

<p>We must now test our new ABINIT binaries. Refer to the ABINIT
<a href="http://www.abinit.org/doc/helpfiles/for-v8.0/install_notes/install.html#make_tests">documentation</a>
for more information on testing. In the same build directory, create a
temporary directory for running the tests.</p>

<div class="highlight highlight-source-shell"><pre>mkdir -p temp <span class="pl-k">&amp;&amp;</span> <span class="pl-c1">cd</span> temp</pre></div>

<p>You can use the <code>$ABINIT/tests/runtests.py</code> program to thoroughly test your new
ABINIT build. Start off with</p>

<div class="highlight highlight-source-shell"><pre>../../tests/runtests.py fast</pre></div>

<p>to run the <code>fast</code> test suite, which are the most basic functionality tests. You
can also run several tests simultaneously,</p>

<div class="highlight highlight-source-shell"><pre>../../tests/runtests.py -j8 fast</pre></div>

<p>which will run 8 tests at once. Some tests are for testing parallelization, and
are designed to run with MPI threads. For instance,</p>

<div class="highlight highlight-source-shell"><pre>../../tests/runtests.py -n4 paral</pre></div>

<p>will run the <code>paral</code> test suite using 4 MPI processes. Some tests may be skipped
depending on the number of processors chosen, so you can try a few different
values. To run the entire battery of tests, run the <code>runtests.py</code> script without
specifying any specific test suite. I suggest something like</p>

<div class="highlight highlight-source-shell"><pre>../../tests/runtests.py -j3 -n4</pre></div>

<p>that will run the entire test suite, 3 tests at a time using up to 4 MPI
processes. This means that there will be between 3 and 12 processes running at a
given time. This will run over 600 tests. Many tests will be skipped since we
did not build all ABINIT features. It is also normal that a few tests will fail,
especially in the <code>v7</code> test suite that contains tests some experimental
connectors. Overall, your test failure rate should be less than 2%. The script
will output the test summary that you can review in the <code>Test_suite</code> directory.
You can view <code>suite_report.html</code> in your web browser for a very comprehensive
report on the tests.</p>

<h3>
<a id="step-4---installation-and-use" class="anchor" href="#step-4---installation-and-use" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Step 4 - Installation and use</h3>

<p><strong>Once you are satisfied with the testing, install ABINIT with <code>make install</code>.</strong></p>

<p>You can run ABINIT in the standard fashion for MPI binaries. For instance,</p>

<pre><code>mpiexec.hydra -np 192 -hosts fat1,fat2,fat3 ~/bin/abinit &lt; some.files
</code></pre>

<p>will run across nodes <code>fat1</code>, <code>fat2</code>, and <code>fat3</code> using 192 MPI processes (64
each). <strong>It is no longer necessary to manually open and close an MPI ring when
initiating MPI parallelization with <code>mpiexec.hydra</code>.</strong> See <code>I_MPI_FABRICS</code> if
you wish to select the
<a href="https://software.intel.com/en-us/node/535584">network interface</a>.</p>

<h2>
<a id="about-optimization" class="anchor" href="#about-optimization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>About optimization</h2>

<p>In general, compiler flags for optimizing on Medusa will look like</p>

<div class="highlight highlight-source-shell"><pre>-axCORE-AVX2,SSE4.2 -ip -static-intel -fp-model precise -fp-model <span class="pl-c1">source</span> -fma</pre></div>

<p>These options enable the binaries to take advantage of the special processor
instructions available for the <code>hexa</code> (SSE4.2) and <code>fat</code> (CORE-AVX2) nodes (the
<code>medusa</code> node is also CORE-AVX2 enabled). There are downsides to this; mainly:
build times are 3 to 4 times longer than standard optimization, and the produced
binaries will NOT function on processors that do not have these instructions,
and will not likely work on any non-Intel processors. Obviously, these downsides
are negligible compared to the tremendous gains in speed and precision that can
be achieved with this level of optimization. However, please refer to the
<a href="https://software.intel.com/en-us/article%0As/performance-tools-for-software-developers-intel-compiler-options-for-sse-gener%0Aation-and-processor-specific-optimizations">official</a> <a href="https://software.int%0Ael.com/en-us/articles/performance-tools-for-software-developers-sse-generation-a%0And-processor-specific-optimizations-continue">documentation</a> for details about these options.</p>

<p>That being said, sometimes the software will not compile properly with these
options, so you should always check these flags and rigorously test after each
build. While troubleshooting and testing, I recommend a safer level of
optimization such as <code>-O2</code>.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/roguephysicist">roguephysicist</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
