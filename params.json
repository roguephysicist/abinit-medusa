{
  "name": "Abinit-medusa",
  "tagline": "A guide for building ABINIT on Medusa",
  "body": "Instructions for building ABINIT on Medusa\r\n=======================================================\r\n\r\nThis is a detailed guide for building highly optimized binaries for ABINIT on\r\nMedusa. We will make extensive use of the Intel MPI and MKL libraries. This\r\nstreamlines the process considerably; combined with processor-specific\r\noptimization, the resulting binaries are are extremely fast without sacrificing\r\naccuracy.\r\n\r\nAs the Intel MPI and MKL libraries are already installed and optimized for the\r\nsystem, we do not need to build any intermediate software. If you choose not to\r\nuse these libraries, then you will have to build your own external libraries for\r\nFFTW, LINALG, and MPI, or default to the internal connectors that come with\r\nABINIT. However, this is outside the scope of this guide.\r\n\r\n\r\nProcedure\r\n-------------------------------------------------------\r\n\r\nWe will use the adjoining `abinit-x.x.x.ac` configure files. Choose the\r\nappropriate one for your version of ABINIT. This file will greatly facilitate\r\nthe build process as it contains all the correct build options and paths. Review\r\nthis file carefully, as it contains a lot of useful information. If you wish to\r\nadd more connectors or experiment with other options, I recommend doing so using\r\nthis file as most of the variables are extensively documented.\r\n\r\n**Typically, the only thing you will need to modify is the `prefix`. Set this to\r\nwherever you want to install ABINIT.**\r\n\r\n\r\n### Step 0 - Prepare the build environment\r\n\r\nYou can compile from any node in Medusa. The appropriate environment should be\r\nloaded by default for common users in the `medusa`, `fat`, and `hexa` nodes.\r\nVerify this by checking your `$PATH`. You should see the paths for the compiler\r\nbinaries (`.../linux/bin/intel64`) and the MPI binaries\r\n(`.../linux/mpi/intel64/bin)`. Verify that you are able to execute `mpiifort`\r\ndirectly. Lastly, verify that the `${MKLROOT}` and `${I_MPI_ROOT}` paths are\r\ncorrect, and match those in your `$PATH`.\r\n\r\n*If you want to build ABINIT system-wide, you will need to build as the\r\nsuper-user. By default, entering `root` does not have the proper environment\r\nloaded. Load it with*\r\n\r\n```sh\r\nmodule load intel-compilers/16.2.181 intel-mpi/5.1.3.181 intel-mkl/16.2.181\r\n```\r\n\r\n*When compiling as root, it is best to only compile from the master node.*\r\n\r\nIt is convenient to have the following variables:\r\n\r\n```sh\r\nCC=\"icc\"\r\nCXX=\"icpc\"\r\nFC=\"ifort\"\r\nF77=\"ifort\"\r\n```\r\n\r\nCheck these and set them if not.\r\n\r\n\r\n### Step 1 - Prepare the source code\r\n\r\nYou can find the tarballs for ABINIT and many other programs in\r\n`/opt/science/tarballs/`.\r\n\r\nExtract the ABINIT source code.\r\n\r\n```sh\r\ntar -xvf /opt/science/tarballs/abinit-7.10.5.tar.gz\r\ncd abinit-7.10.5\r\n```\r\n\r\nWe will modify the configure script to match our new compilers. This will\r\nsuppress many useless notifications and allow the build system to correctly set\r\nthe MPI compilers.\r\n\r\n```sh\r\nsed -i -e 's/vec-report0/qopt-report=0/g' \\\r\n       -e 's/mpicc/mpiicc/g' \\\r\n       -e 's/mpicxx/mpiicpc/g' \\\r\n       -e 's/mpif90/mpiifort/g' configure\r\n```\r\n\r\nI recommend creating a new directory for building. This makes for easier clean\r\nup and for testing different compile options.\r\n\r\n```\r\nmkdir -p build && cd build\r\n```\r\n\r\nCopy the appropriate `abinit-x.x.x.ac` file to this directory, and rename it to\r\n`medusa.ac` or whatever the hostname is of the node your are in.\r\n\r\n```sh\r\ncp abinit-x.x.x.ac ${HOSTNAME}.ac\r\n```\r\n\r\n\r\n### Step 2 - Configure and build\r\n\r\nInitiate the configuration process by running\r\n\r\n```sh\r\n../configure\r\n```\r\n\r\nThe script will read from the `.ac` file and set up the build system\r\naccordingly. Review the process carefully and check for any mistakes or\r\nproblems. If everything goes as planned, it should produce this summary:\r\n\r\n```\r\nSummary of important options:\r\n\r\n  * C compiler      : intel version 16.0\r\n  * Fortran compiler: intel version 16.0\r\n  * architecture    : intel xeon (64 bits)\r\n\r\n  * debugging       : basic\r\n  * optimizations   : yes\r\n\r\n  * OpenMP enabled  : no (collapse: ignored)\r\n  * MPI    enabled  : yes\r\n  * MPI-IO enabled  : yes\r\n  * GPU    enabled  : no (flavor: none)\r\n\r\n  * TRIO   flavor = none\r\n  * TIMER  flavor = abinit (libs: ignored)\r\n  * LINALG flavor = mkl (libs: auto-detected)\r\n  * ALGO   flavor = none (libs: ignored)\r\n  * FFT    flavor = fftw3-mkl (libs: auto-detected)\r\n  * MATH   flavor = none (libs: ignored)\r\n  * DFT    flavor = none\r\n```\r\n\r\nYou are now ready to build the software. You should parallelize this process to\r\nsignificantly reduce compile time. For instance,\r\n\r\n```sh\r\nmake multi multi_nprocs=8\r\n```\r\n\r\nwill use 8 processor cores. **For this level of optimization, compiling will\r\ntake roughly 35 minutes on 8 cores.**\r\n\r\n\r\n### Step 3 - Testing\r\n\r\nWe must now test our new ABINIT binaries. Refer to the ABINIT\r\n[documentation]\r\n(http://www.abinit.org/doc/helpfiles/for-v8.0/install_notes/install.html#make_tests)\r\nfor more information on testing. In the same build directory, create a\r\ntemporary directory for running the tests.\r\n\r\n```sh\r\nmkdir -p temp && cd temp\r\n```\r\n\r\nYou can use the `$ABINIT/tests/runtests.py` program to thoroughly test your new\r\nABINIT build. Start off with\r\n\r\n```sh\r\n../../tests/runtests.py fast\r\n```\r\n\r\nto run the `fast` test suite, which are the most basic functionality tests. You\r\ncan also run several tests simultaneously,\r\n\r\n```sh\r\n../../tests/runtests.py -j8 fast\r\n```\r\n\r\nwhich will run 8 tests at once. Some tests are for testing parallelization, and\r\nare designed to run with MPI threads. For instance,\r\n\r\n```sh\r\n../../tests/runtests.py -n4 paral\r\n```\r\n\r\nwill run the `paral` test suite using 4 MPI processes. Some tests may be skipped\r\ndepending on the number of processors chosen, so you can try a few different\r\nvalues. To run the entire battery of tests, run the `runtests.py` script without\r\nspecifying any specific test suite. I suggest something like\r\n\r\n```sh\r\n../../tests/runtests.py -j3 -n4\r\n```\r\n\r\nthat will run the entire test suite, 3 tests at a time using up to 4 MPI\r\nprocesses. This means that there will be between 3 and 12 processes running at a\r\ngiven time. This will run over 600 tests. Many tests will be skipped since we\r\ndid not build all ABINIT features. It is also normal that a few tests will fail,\r\nespecially in the `v7` test suite that contains tests some experimental\r\nconnectors. Overall, your test failure rate should be less than 2%. The script\r\nwill output the test summary that you can review in the `Test_suite` directory.\r\nYou can view `suite_report.html` in your web browser for a very comprehensive\r\nreport on the tests.\r\n\r\n\r\n### Step 4 - Installation and use\r\n\r\n**Once you are satisfied with the testing, install ABINIT with `make install`.**\r\n\r\nYou can run ABINIT in the standard fashion for MPI binaries. For instance,\r\n\r\n```\r\nmpiexec.hydra -np 192 -hosts fat1,fat2,fat3 ~/bin/abinit < some.files\r\n```\r\n\r\nwill run across nodes `fat1`, `fat2`, and `fat3` using 192 MPI processes (64\r\neach). **It is no longer necessary to manually open and close an MPI ring when\r\ninitiating MPI parallelization with `mpiexec.hydra`.** See `I_MPI_FABRICS` if\r\nyou wish to select the\r\n[network interface](https://software.intel.com/en-us/node/535584).\r\n\r\nAbout optimization\r\n-------------------------------------------------------\r\n\r\nIn general, compiler flags for optimizing on Medusa will look like\r\n\r\n```sh\r\n-axCORE-AVX2,SSE4.2 -ip -static-intel -fp-model precise -fp-model source -fma\r\n```\r\n\r\nThese options enable the binaries to take advantage of the special processor\r\ninstructions available for the `hexa` (SSE4.2) and `fat` (CORE-AVX2) nodes (the\r\n`medusa` node is also CORE-AVX2 enabled). There are downsides to this; mainly:\r\nbuild times are 3 to 4 times longer than standard optimization, and the produced\r\nbinaries will NOT function on processors that do not have these instructions,\r\nand will not likely work on any non-Intel processors. Obviously, these downsides\r\nare negligible compared to the tremendous gains in speed and precision that can\r\nbe achieved with this level of optimization. However, please refer to the\r\n[official](https://software.intel.com/en-us/article\r\ns/performance-tools-for-software-developers-intel-compiler-options-for-sse-gener\r\nation-and-processor-specific-optimizations) [documentation](https://software.int\r\nel.com/en-us/articles/performance-tools-for-software-developers-sse-generation-a\r\nnd-processor-specific-optimizations-continue) for details about these options.\r\n\r\nThat being said, sometimes the software will not compile properly with these\r\noptions, so you should always check these flags and rigorously test after each\r\nbuild. While troubleshooting and testing, I recommend a safer level of\r\noptimization such as `-O2`.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}